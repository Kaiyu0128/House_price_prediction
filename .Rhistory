#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ ., data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ . -renovated, data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ ., data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ ., data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ . -renovated, data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ . -renovated, data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ . -renovated, data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ . -renovated, data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ bedrooms + bathrooms + sqft_living + floors +
condition + yr_built + I(log(sqft_lot)), data = seattle.df, method = "lm", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ . -renovated, data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
#################
# The Lasso  ####
#################
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)#coefficient plot
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)#CV error plot
bestlam=cv.out$lambda.min #cv.out$lambda.1se
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)#compare to OLS error
sqrt(mean((lasso.pred-y.test)^2))
#refit on the full data set, using the "best" lambda
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
lasso.coef
#refit on the full data set, using the "best" lambda
set.seed(1)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ ., data = seattle.df, method = "lasso", trControl = ctrl)
#view summary of k-fold CV
print(model)
#######################
# Ridge Regression  ###
#######################
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid)
plot(lasso.mod)#coefficient plot
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)#CV error plot
bestlam=cv.out$lambda.min #cv.out$lambda.1se
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-price.test)^2) #compare to OLS error
sqrt(mean((ridge.pred-price.test)^2))
#refit on the full data set, using the "best" lambda
out=glmnet(x,y,alpha=0,lambda=grid)
ridge.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
ridge.coef
ridge.coef[ridge.coef!=0]
#######################
# Ridge Regression  ###
#######################
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid)
plot(ridge.mod)#coefficient plot
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)#CV error plot
bestlam=cv.out$lambda.min #cv.out$lambda.1se
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2) #compare to OLS error
sqrt(mean((ridge.pred-y.test)^2))
#refit on the full data set, using the "best" lambda
out=glmnet(x,y,alpha=0,lambda=grid)
#refit on the full data set, using the "best" lambda
set.seed(1)
out=glmnet(x,y,alpha=0,lambda=grid)
ridge.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
ridge.coef
ridge.coef[ridge.coef!=0]
ridge.coef
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ ., data = seattle.df, method = "ridge", trControl = ctrl)
#view summary of k-fold CV
print(model)
#data <- read.csv('/Users/kaiyuyokoi/Desktop/House_price_prediction/Seattle.csv')
data <- read.csv('/Users/linw/Desktop/House_price_prediction/Seattle.csv')
data$renovated = as.factor(data$renovated)
summary(data)
str(data)
# Training and test sets
# Set a seed for reproducibility
set.seed(1)
# Determine the number of rows in the dataset
n <- nrow(data)
# Determine the number of rows for the training set (e.g., 80% of the data)
n_train <- round(0.80 * n)
# Randomly select rows for the training set
train_indices <- sample(1:n, n_train)
# Create the training set
data_train <- data[train_indices, ]
# Create the test set
data_test <- data[-train_indices, ]
# k-fold cross-validation
# to select optimal degree d for the polynomial
# variable: floors
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(floors,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# Plot of resulting polynomial fit to the data
xlims=range(data$floors)
#create a grid of values for x-variable at which we want predictions
x.grid=seq(from=xlims[1],to=xlims[2])
#se=T indicates that we want standard errors as well.
preds=predict(glm(price~poly(floors,2), data=data),newdata=list(floors=x.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#plot the data and add polynomial fit.
plot(data$floors,data$price,xlim=xlims,cex=.5,col="darkgrey")
lines(x.grid,preds$fit,lwd=2,col="blue")
title("Degree-2 Polynomial",outer=T)
lines(x.grid,preds$fit,lwd=2,col="blue")
matlines(x.grid,se.bands,lwd=1,col="blue",lty=3)
# CV for optimal degree
# variable: condition
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(condition,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# CV for optimal degree
# variable: condition
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(bedrooms,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# CV for optimal degree
# variable: condition
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(bathrooms,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# CV for optimal degree
# variable: condition
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(sqft_living,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# CV for optimal degree
# variable: condition
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(sqft_lot,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# CV for optimal degree
# variable: sqft_living
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(sqft_living,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# Plot of resulting polynomial fit to the data
xlims=range(data$condition)
#create a grid of values for x-variable at which we want predictions
x.grid=seq(from=xlims[1],to=xlims[2])
#se=T indicates that we want standard errors as well.
preds=predict(glm(price~poly(condition,4), data=data),newdata=list(condition=x.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#plot the data and add polynomial fit.
plot(data$condition,data$price,xlim=xlims,cex=.5,col="darkgrey")
lines(x.grid,preds$fit,lwd=2,col="blue")
title("Degree-4 Polynomial",outer=T)
lines(x.grid,preds$fit,lwd=2,col="blue")
matlines(x.grid,se.bands,lwd=1,col="blue",lty=3)
# Plot of resulting polynomial fit to the data
xlims=range(data$sqft_living)
#create a grid of values for x-variable at which we want predictions
x.grid=seq(from=xlims[1],to=xlims[2])
#se=T indicates that we want standard errors as well.
preds=predict(glm(price~poly(sqft_living,2), data=data),newdata=list(sqft_living=x.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#plot the data and add polynomial fit.
plot(data$sqft_living,data$price,xlim=xlims,cex=.5,col="darkgrey")
lines(x.grid,preds$fit,lwd=2,col="blue")
title("Degree-4 Polynomial",outer=T)
lines(x.grid,preds$fit,lwd=2,col="blue")
matlines(x.grid,se.bands,lwd=1,col="blue",lty=3)
# CV for optimal d
# variable: yr_built
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(yr_built,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
View(seattle.df)
# CV for optimal degree
# variable: sqft_lot
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(sqft_lot,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# CV for optimal degree
# variable: sqft_living
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(sqft_living,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# Plot of resulting polynomial fit to the data
xlims=range(data$sqft_living)
#create a grid of values for x-variable at which we want predictions
x.grid=seq(from=xlims[1],to=xlims[2])
#se=T indicates that we want standard errors as well.
preds=predict(glm(price~poly(sqft_living,2), data=data),newdata=list(sqft_living=x.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#plot the data and add polynomial fit.
plot(data$sqft_living,data$price,xlim=xlims,cex=.5,col="darkgrey")
lines(x.grid,preds$fit,lwd=2,col="blue")
title("Degree-4 Polynomial",outer=T)
lines(x.grid,preds$fit,lwd=2,col="blue")
matlines(x.grid,se.bands,lwd=1,col="blue",lty=3)
# CV for optimal degree
# variable: sqft_living
library(boot)
set.seed(1)
cv.error.10=rep(0,4)
for (i in 1:4){
glm.fit=glm(price~poly(sqft_living,i), data=data)
cv.error.10[i]=cv.glm(data,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type = "b", xlab="Degree", ylab="CV Error")
# Plot of resulting polynomial fit to the data
xlims=range(data$sqft_living)
#create a grid of values for x-variable at which we want predictions
x.grid=seq(from=xlims[1],to=xlims[2])
#se=T indicates that we want standard errors as well.
preds=predict(glm(price~poly(sqft_living,2), data=data),newdata=list(sqft_living=x.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#plot the data and add polynomial fit.
plot(data$sqft_living,data$price,xlim=xlims,cex=.5,col="darkgrey")
lines(x.grid,preds$fit,lwd=2,col="blue")
title("Degree-4 Polynomial",outer=T)
lines(x.grid,preds$fit,lwd=2,col="blue")
matlines(x.grid,se.bands,lwd=1,col="blue",lty=3)
# Plot of resulting polynomial fit to the data
xlims=range(data$sqft_living)
#create a grid of values for x-variable at which we want predictions
x.grid=seq(from=xlims[1],to=xlims[2])
#se=T indicates that we want standard errors as well.
preds=predict(glm(price~poly(sqft_living,2), data=data),newdata=list(sqft_living=x.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#plot the data and add polynomial fit.
plot(data$sqft_living,data$price,xlim=xlims,cex=.5,col="darkgrey")
lines(x.grid,preds$fit,lwd=2,col="blue")
title("Degree-2 Polynomial",outer=T)
lines(x.grid,preds$fit,lwd=2,col="blue")
matlines(x.grid,se.bands,lwd=1,col="blue",lty=3)
# Plot of resulting polynomial fit to the data
xlims=range(data$yr_built)
#create a grid of values for x-variable at which we want predictions
x.grid=seq(from=xlims[1],to=xlims[2])
#se=T indicates that we want standard errors as well.
preds=predict(glm(price~poly(yr_built,2),data=data),newdata=list(yr_built=x.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#plot the data and add polynomial fit.
plot(data$yr_built,data$price,xlim=xlims,cex=.5,col="darkgrey")
lines(x.grid,preds$fit,lwd=2,col="blue")
title("Degree-2 Polynomial",outer=T)
lines(x.grid,preds$fit,lwd=2,col="blue")
matlines(x.grid,se.bands,lwd=1,col="blue",lty=3)
fit.poly = lm(log(price) ~ bedrooms + bathrooms + poly(sqft_living,2) + sqft_lot + poly(floors,2) + condition + poly(yr_built, 2) + renovated, data = data_train)
summary(fit.poly)
fit_log.poly = lm(log(price) ~ bedrooms + bathrooms + poly(sqft_living,2) + sqft_lot + poly(floors,2) + condition + poly(yr_built, 2) + renovated, data = data_train)
summary(fit_log.poly)
library(MASS)
stepAIC(fit_log.poly)
fit_log.poly2 = lm(formula = log(price) ~ bedrooms + bathrooms + poly(sqft_living,
2) + sqft_lot + poly(floors, 2) + condition + poly(yr_built,
2) + renovated, data = data_train)
summary(fit.poly2)
fit_log.poly2 = lm(formula = log(price) ~ bedrooms + bathrooms + poly(sqft_living,
2) + sqft_lot + poly(floors, 2) + condition + poly(yr_built,
2) + renovated, data = data_train)
summary(fit_log.poly2)
# Remove insignificant predictors
fit_log.poly3 = lm(formula = log(price) ~ bedrooms + bathrooms + sqft_living +
sqft_lot + floors + condition + yr_built, data = data_train)
summary(fit.poly3)
# Remove insignificant predictors
fit_log.poly3 = lm(formula = log(price) ~ bedrooms + bathrooms + sqft_living +
sqft_lot + floors + condition + yr_built, data = data_train)
summary(fit_log.poly3)
# Likelihood ratio test
anova(fit_log.poly3, fit_log.poly, test= "LRT")
# Remove insignificant predictors
fit_log.poly3 = lm(formula = log(price) ~ bedrooms + bathrooms + sqft_living +
sqft_lot + floors + condition + yr_built +renovated, data = data_train)
summary(fit_log.poly3)
# Likelihood ratio test
anova(fit_log.poly3, fit_log.poly, test= "LRT")
# Remove insignificant predictors
fit_log.poly3 = lm(formula = log(price) ~ bedrooms + bathrooms + sqft_living +
sqft_lot + floors + condition + yr_built, data = data_train)
summary(fit_log.poly3)
# Likelihood ratio test
anova(fit_log.poly3, fit_log.poly, test= "LRT")
# Remove insignificant predictors
fit_log.poly3 = lm(formula = log(price) ~ bedrooms + bathrooms + poly(sqft_living,
2) + sqft_lot + floors + condition + yr_built, data = data_train)
summary(fit_log.poly3)
# Likelihood ratio test
anova(fit_log.poly3, fit_log.poly, test= "LRT")
# Likelihood ratio test comparing linear model and polynomial model
anova(lm(formula = log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot +
floors + condition + yr_built,
data = data_train),fit_log.poly3, test= "LRT")
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ bedrooms + bathrooms + poly(sqft_living,2) +
poly(floors,2) + sqft_lot + condition + yr_built + , data = seattle.df, method = "lm", trControl = ctrl)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ bedrooms + bathrooms + poly(sqft_living, 2) + sqft_lot +
floors + condition + yr_built, data = seattle.df, method = "lm", trControl = ctrl)
#view summary of k-fold CV
print(model)
library(caret)
#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ bedrooms + bathrooms + poly(sqft_living, 2) + sqft_lot +
floors + condition + yr_built, data = seattle.df, method = "lm", trControl = ctrl)
#view summary of k-fold CV
print(model)
