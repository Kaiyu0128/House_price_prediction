---
title: "House Price Prediction 1: Linear Regression"
output:
  word_document: default
  html_document: default
date: "2023-05-18"
---

### Introduction
In this file, we will use a multiple linear regression model to predict housing prices in Seattle, Washington in 2014. The data we will use is from Kaggle, uploaded by SHREE. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues. Additionally, we will model the data using random forest and cubic spline. The full dataset contains 4600 houses throughout the state of Washington with 18 variables, 1573 of the houses belonging to Seattle (1561 after data cleaning). The response variable is log(price). 

### Summary of the data set
```{r}
data <- read.csv('~/Desktop/House_Price_Prediction/data.csv',
                 stringsAsFactors = TRUE)
str(data)
summary(data)

# Response variable: price
hist(data$price)
```

```{r}
# Display prices below $7 million
hist(data$price[which(data$price < 7000000)])
```

### Data cleaning
We immediately remove the observations with price = 0. The variables sqft_above and sqft_basement will be dropped, since their sum is the variable sqft_living. Also, we do not know the meaning of the view variable, so we choose to exclude it from the model. Next, we drop the following variables: date, street, statezip, and country. Including zip codes as factors would increase the number of variables greatly, so we decide not to consider them in the model. The variable yr_renovated is converted into the variable "renovated" with levels 1 = Yes and 0 = No. Finally, renovated and waterfront are converted into factor variables.

Once the data is cleaned, we will take a subset of only the Seattle entries.
```{r}
data2 <- subset(data, price > 0)
data2 <- subset(data2, select = -c(sqft_above,sqft_basement, view, 
                                   date, street, statezip, country)) 

data2$yr_renovated[data2$yr_renovated>0] <- 1
names(data2)[names(data2) == "yr_renovated"] <- "renovated"

data2$waterfront <- as.factor(data2$waterfront)
data2$renovated <- as.factor(data2$renovated)

summary(data2)
```

We note that waterfront is unbalanced, with a much greater proportion of 0s than 1s. Now we clean the response variable: price. Immediately there is a low outlier of 7800 to remove.

```{r}
# Low outlier and two high outliers identified manually
data2 <- subset(data2, price != 7800 & price != 26590000 & price != 12899000) 
boxplot(data2$price)
summary(data2$price)
```
To recap, 50 low outliers which we believe are bad data have been removed. Two high outliers have been removed.

Now that the Washington data is clean, we are ready to analyze the Seattle data.

### Exploratory Data Analysis

We analyze the remaining predictors in the Seattle data set.

```{r}
# Create Seattle data frame
seattle.df = subset(data2, city == "Seattle")
seattle.df = subset(seattle.df, select = -c(city))
dim(seattle.df)
summary(seattle.df)
```

There is only one waterfront = 1 entry, so we will remove the waterfront variable in the model. 

```{r}
# Drop waterfront
seattle.df = subset(seattle.df, select = -c(waterfront))
```

Now we have 8 numerical predictors and 1 categorical predictor. The Seattle data frame consists of 1561 houses with 9 predictors. 

```{r}
# Price and log(price)
boxplot(seattle.df$price)
summary(seattle.df$price)
hist(seattle.df$price)
plot(density(seattle.df$price))

boxplot(log(seattle.df$price))
summary(log(seattle.df$price))
hist(log(seattle.df$price))
plot(density(log(seattle.df$price)))
```
From the numerical summary: the minimum house price in Seattle is 90,000, the median is 490,000, the mean is 576401, and the maximum is 3,200,000. Price is heavily skewed right.

```{r}
hist(seattle.df$bedrooms)
hist(seattle.df$bathrooms)
hist(seattle.df$sqft_living)
hist(seattle.df$sqft_lot)
hist(seattle.df$floors)
hist(seattle.df$condition)
hist(seattle.df$yr_built)
plot(seattle.df$renovated, col = c("red","blue"), main = "Renovated", ylab = "Count")
```

sqft_lot is extremely right skewed, so we will use a log-transformation on sqft_lot.

```{r}
# Relationships between response and predictors
par(mfrow = c(2, 2))
plot(price ~ bedrooms, data = seattle.df, main="Price vs Bedrooms", lwd=2)
  abline(lm(price~bedrooms, data=seattle.df),col="red",lwd=2)
plot(price ~ bathrooms, data = seattle.df, main="Price vs Bathrooms", lwd=2)
  abline(lm(price~bathrooms, data=seattle.df),col="red",lwd=2)
plot(price ~ sqft_living, data = seattle.df, main="Price vs Sqft_living", lwd=2)
  abline(lm(price~sqft_living, data=seattle.df),col="red",lwd=2)
plot(price ~ sqft_lot, data = seattle.df, main="Price vs Sqft_lot", lwd=2)
  abline(lm(price~sqft_lot, data=seattle.df),col="red",lwd=2)
plot(price ~ floors, data = seattle.df, main="Price vs Floors", lwd=2)
  abline(lm(price~floors, data=seattle.df),col="red",lwd=2)
plot(price ~ condition, data = seattle.df, main="Price vs Condition", lwd=2)
  abline(lm(price~condition, data=seattle.df),col="red",lwd=2)
plot(price ~ yr_built, data = seattle.df, main="Price vs Year built", lwd=2)
  abline(lm(price~yr_built, data=seattle.df),col="red",lwd=2)
plot(price ~ renovated, data = seattle.df, main="Price vs Renovated", lwd=2)
par(mfrow = c(1, 1))
hist(seattle.df$price[which(seattle.df$renovated == 1)])
hist(seattle.df$price[which(seattle.df$renovated == 0)])
```
```{r}
summary(seattle.df)
str(seattle.df)
```

Price appears positively correlated with all predictors except yr_built. The distributions of price with renovated = 1 and renovated = 0 seem to be the same.

### Creating the model

#### Splitting data into training and test sets
```{r}
library(ggplot2)
set.seed(1)

# Training and test sets
train = sample(1:nrow(seattle.df), nrow(seattle.df)*0.8) # 80/20 split
test = (-train)
price.test = seattle.df$price[test]

# Dataframes
train.df = data.frame(seattle.df[train,])
test.df = data.frame(seattle.df[test,])
```


#### Full fit of the model
```{r}
seattle_full.lm = lm(log(price) ~ ., data=seattle.df, subset=train)
summary(seattle_full.lm)

# Test MSE 
lm_full.pred = predict(seattle_full.lm, test.df, type=c("response"))
err.lm_full = mean((lm_full.pred-log(test.df$price))^2)
err.lm_full
rmse.lm.full = sqrt(err.lm_full)
rmse.lm.full
```

The full model has an R2 = 0.5746. After fitting the linear regression model, we find that the variable renovated is not significant.

### Variable Selection

#### 1. Statistical significance selection (P-value selection)
```{r}
seattle.lm2 = update(seattle_full.lm, ~ . -renovated) 
summary(seattle.lm2)

#Test MSE 
lm.pred = predict(seattle.lm2, test.df, type=c("response"))
err.lm = mean((lm.pred-log(test.df$price))^2)
err.lm
rmse.lm = sqrt(err.lm)
rmse.lm
```



##### Model Adequacy Checking
Assumptions for linear regression: 1) little to no multicollinearity, 2) linear relationship between predictors and response, 3) normally-distributed variables, and 4) variances are equal.

```{r}
# Checking multicollinearity
pairs(seattle.df)
cor(seattle.df[,c("price","bedrooms","bathrooms", "sqft_living", "sqft_lot","floors", "condition", "yr_built")])
```

Since every coefficient is less than 0.75, we will assume there is little multicollinearity and leave the variables as they are. Sqft_living and bathrooms seem to be the best individual predictors of price, with correlation coefficients of 0.75 and 0.52. Also, sqft_living and bathrooms have the highest correlation among the predictors with a coefficient of 0.69.

```{r}
```

Since sqft_lot was skewed in our exploratory data analysis, we will apply a log-transform to sqft_lot in the next model. 

Now we check the residuals.

```{r}
# Checking residuals
plot(seattle.lm2)
```

Now we conduct a likelihood ratio test to compare nested models.

```{r}
# Likelihood ratio test
anova(seattle.lm2, seattle_full.lm, test="LRT")
```

With a p-value of 0.198 > 0.05 we fail to reject H0, so it is safe to assume the simpler model and the full model are the same.

Now we select a model using backward subset selection.

#### 2. Backward subset selection
```{r}
# Backward subset selection
library(MASS)
stepAIC(seattle_full.lm) # stepwise backward selection using AIC
```

Backward subset selection removes one predictors renovated. Thus the number of predictors is the same as if we had removed based on p-value. Now we will log-transform the necessary predictor, and use this model as our linear regression model for prediction.

### Final linear regression model
```{r}
summary(seattle.lm2)

# Test MSE 
lm.pred = predict(seattle.lm2, test.df, type=c("response"))
err.lm = mean((lm.pred-log(test.df$price))^2)
err.lm
rmse = sqrt(err.lm)
rmse
```

```{r}
# Plot of Actual vs. Predicted House Log-Prices
plot(lm.pred, log(test.df$price), xlab="Predicted log price", ylab="Actual log price", main = "Actual vs. Predicted House Log-Price")
abline(lm(log(test.df$price)~lm.pred,
          data=train.df),col="red",lwd=2)
```

### Cross-validation metrics
```{r}
library(caret)

#specify the cross-validation method
# k-fold CV; k=10
set.seed(1)
ctrl <- trainControl(method = "cv", number = 10)

#fit a regression model and use k-fold CV to evaluate performance
model <- train(log(price) ~ bedrooms + bathrooms + sqft_living + floors + 
    condition + yr_built + I(log(sqft_lot)), data = seattle.df, method = "lm", trControl = ctrl)

#view summary of k-fold CV               
print(model)
```

### Conclusion
Cross-validated RMSE = 0.3244248, R2 = 0.5686078 , MAE = 0.2463774.