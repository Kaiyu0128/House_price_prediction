---
title: "House Price Prediction 1: Linear Regression"
output:
  word_document: default
  html_document: default
date: "2023-05-18"
---

### Introduction
In this file, we will use a multiple linear regression model to predict housing prices in Seattle, Washington in 2014. The data we will use is from Kaggle, uploaded by SHREE. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues. Additionally, we will model the data using random forest and cubic spline. The full dataset contains 4600 houses throughout the state of Washington with 18 variables, 1573 of the houses belonging to Seattle (1461 after data cleaning). The response variable is price. 

### Summary of the data set
```{r}
data <- read.csv('~/Desktop/House_Price_Prediction/data.csv',
                 stringsAsFactors = TRUE)
str(data)
summary(data)

# Response variable: price
hist(data$price)
```

```{r}
# Display prices below $7 million
hist(data$price[which(data$price < 7000000)])
```

### Data cleaning
We immediately remove the observations with price = 0. The variables sqft_above and sqft_basement will be dropped, since their sum is the variable sqft_living. Also, we do not know the meaning of the view variable, so we choose to exclude it from the model. Next, we drop the following variables: date, street, statezip, and country. Including zip codes as factors would increase the number of variables greatly, so we decide not to consider them in the model. The variable yr_renovated is converted into the variable "renovated" with levels 1 = Yes and 0 = No. Finally, renovated and waterfront are converted into factor variables.

Once the data is cleaned, we will take a subset of only the Seattle entries.
```{r}
data2 <- subset(data, price > 0)
data2 <- subset(data2, select = -c(sqft_above,sqft_basement, view, 
                                   date, street, statezip, country)) 

data2$yr_renovated[data2$yr_renovated>0] <- 1
names(data2)[names(data2) == "yr_renovated"] <- "renovated"

data2$waterfront <- as.factor(data2$waterfront)
data2$renovated <- as.factor(data2$renovated)

summary(data2)
```

We note that waterfront is unbalanced, with a much greater proportion of 0s than 1s. Now we clean the response variable: price.

```{r}
data2 <- subset(data2, price != 7800) #Low outlier found
boxplot(data2$price) #with outliers
#identify and remove outliers. R does this using the 1.5*IQR criterion
outliers <- boxplot(data2$price, plot=FALSE)$out
data2 <- data2[-which(data2$price %in% outliers),]
```
```{r}
hist(data2$price)
plot(density(data2$price))
plot(density(log(data2$price)))
```

Essentially, the top 15% of Washing house prices have been removed by using the 1.5*IQR criterion, and we remove one low outlier manually. Now that the Washington data is clean, we are ready to analyze the Seattle data.

### Exploratory Data Analysis

We analyze the remaining predictors in the Seattle data set.

```{r}
# Create Seattle data frame
seattle.df = subset(data2, city == "Seattle")
seattle.df = subset(seattle.df, select = -c(city))
dim(seattle.df)
summary(seattle.df)
```

There is only one waterfront = 1 entry, so we will remove the waterfront variable in the model. 

```{r}
# Drop waterfront
seattle.df = subset(seattle.df, select = -c(waterfront))
```

Now we have 8 numerical predictors and 1 categorical predictor. The Seattle data frame consists of 1461 houses with 9 variables. 

```{r}
# Price
boxplot(seattle.df$price)
summary(seattle.df$price)
hist(seattle.df$price)
plot(density(seattle.df$price))
plot(density(log(seattle.df$price)))
```
From the summary: the minimum house price in Seattle is 90,000, the median is 475,000, the mean is 509351, and the maximum is 1,145,000. We notice that price is bell-shaped but is skewed right, so we will check the fit for log(price) along with price.

```{r}
hist(seattle.df$bedrooms)
hist(seattle.df$bathrooms)
hist(seattle.df$sqft_living)
hist(seattle.df$sqft_lot)
hist(seattle.df$floors)
hist(seattle.df$condition)
hist(seattle.df$yr_built)
plot(seattle.df$renovated, col = c("red","blue"), main = "Renovated", ylab = "Count")
```

sqft_lot is extremely right skewed, so we will use a log-transformation on sqft_lot.

```{r}
# Relationships between response and predictors
par(mfrow = c(2, 2))
plot(price ~ bedrooms, data = seattle.df, main="Price vs Bedrooms", lwd=2)
  abline(lm(price~bedrooms, data=seattle.df),col="red",lwd=2)
plot(price ~ bathrooms, data = seattle.df, main="Price vs Bathrooms", lwd=2)
  abline(lm(price~bathrooms, data=seattle.df),col="red",lwd=2)
plot(price ~ sqft_living, data = seattle.df, main="Price vs Sqft_living", lwd=2)
  abline(lm(price~sqft_living, data=seattle.df),col="red",lwd=2)
plot(price ~ sqft_lot, data = seattle.df, main="Price vs Sqft_lot", lwd=2)
  abline(lm(price~sqft_lot, data=seattle.df),col="red",lwd=2)
plot(price ~ floors, data = seattle.df, main="Price vs Floors", lwd=2)
  abline(lm(price~floors, data=seattle.df),col="red",lwd=2)
plot(price ~ condition, data = seattle.df, main="Price vs Condition", lwd=2)
  abline(lm(price~condition, data=seattle.df),col="red",lwd=2)
plot(price ~ yr_built, data = seattle.df, main="Price vs Year built", lwd=2)
  abline(lm(price~yr_built, data=seattle.df),col="red",lwd=2)
plot(price ~ renovated, data = seattle.df, main="Price vs Renovated", lwd=2)
par(mfrow = c(1, 1))
hist(seattle.df$price[which(seattle.df$renovated == 1)])
hist(seattle.df$price[which(seattle.df$renovated == 0)])
```
```{r}
summary(seattle.df)
str(seattle.df)
```

There appears to be no linear relationship between price and sqft_lot, price and yr_built, and price and yr_renovated. Price is positively correlated with all other predictors. 

### Creating the model

#### Splitting data into training and test sets
```{r}
library(ggplot2)
set.seed(1)

# Training and test sets
train = sample(1:nrow(seattle.df), nrow(seattle.df)*0.8) # 80/20 split
test = (-train)
price.test = seattle.df$price[test]

# Dataframes
train.df = data.frame(seattle.df[train,])
test.df = data.frame(seattle.df[test,])
```


#### Full fit of the model
```{r}
seattle_full.lm = lm((price) ~ ., data=seattle.df, subset=train)
summary(seattle_full.lm)

# Test MSE 
lm_full.pred = predict(seattle_full.lm, test.df, type=c("response"))
err.lm_full = mean((lm_full.pred-test.df$price)^2)
err.lm_full
rmse.lm.full = sqrt(err.lm_full)
rmse.lm.full

# Log-transformed price
seattle_full.lmlog = lm(log(price) ~ ., data=seattle.df, subset=train)
summary(seattle_full.lmlog)
```

The price model performs better than the log(price) model (R2 = 0.4846 vs. R2 = 0.4526). After fitting the linear regression model, we find that the variable renovated is not significant.

### Variable Selection

#### 1. Statistical significance selection (P-value selection)
```{r}
seattle.lm2 = update(seattle_full.lm, ~ . -renovated) 
summary(seattle.lm2)

#Test MSE 
lm.pred = predict(seattle.lm2, test.df, type=c("response"))
err.lm = mean((lm.pred-test.df$price)^2)
err.lm
rmse.lm = sqrt(err.lm)
rmse.lm
```

##### Model Adequacy Checking
Assumptions for linear regression: 1) little to no multicollinearity, 2) linear relationship between predictors and response, 3) normally-distributed variables, and 4) variances are equal.

```{r}
# Checking multicollinearity
pairs(seattle.df)
cor(seattle.df[,c("price","bedrooms","bathrooms", "sqft_living", "sqft_lot","floors", "condition", "yr_built")])
```

Since every coefficient is less than 0.75, we will assume there is little multicollinearity and leave the variables as they are. Note: sqft_living and bathrooms seem to be the best individual predictors of price, with correlation coefficients of 0.64 and 0.42.

```{r}
```

Now recall from the analysis of the individual scatterplots of price against the predictors that sqft_lot and yr_built appear to have no linear relationship with price. We apply a log-transformation to these predictors.

```{r}
# Need linear relationship between response and predictors 
# Check if log-transform yields linear relationship
plot(price ~ I(log(sqft_lot)), data = seattle.df, main="Price vs log(sqft_lot)", lwd=2)
  abline(lm(price~I(log(sqft_lot)), data=seattle.df),col="red",lwd=2)
plot(price ~ I(log(yr_built)), data = seattle.df, main="Price vs log(yr_built)", lwd=2)
  abline(lm(price~I(log(yr_built)), data=seattle.df),col="red",lwd=2)
```
The log-transform on yr_built doesn't appear much different. Since sqft_lot was skewed in our exploratory data analysis, we will apply a log-transform to sqft_lot in the next model. 

Now we check the residuals.

```{r}
# Checking residuals
plot(seattle.lm2)
```
There is nothing unusual with the residuals. We can assume the variables are normally distributed and variances are equal.

Now we conduct a likelihood ratio test to compare nested models.

```{r}
# Likelihood ratio test
anova(seattle.lm2, seattle_full.lm, test="LRT")
```

With a p-value of 0.4706 > 0.05 we fail to reject H0, so it is safe to assume the simpler model and the full model are the same.

Now we select a model using backward subset selection.

#### 2. Backward subset selection
```{r}
# Backward subset selection
library(MASS)
stepAIC(seattle_full.lm) # stepwise backward selection using AIC
```

Backward subset selection removes one predictor: yr_renovated. Thus the number of predictors is the same as in model 2. Now we will log-transform the necessary predictor, and use this model as our linear regression model for prediction.

#### 3. Log-transform predictor
```{r}
#Transform sqft_living
seattle.lm3 = update(seattle_full.lm, ~ . - renovated - sqft_lot + I(log(sqft_lot)))
```

### Final linear regression model
```{r}
summary(seattle.lm3)

# Test MSE 
lm.pred = predict(seattle.lm3, test.df, type=c("response"))
err.lm = mean((lm.pred-test.df$price)^2)
err.lm
rmse = sqrt(err.lm)
rmse
```

### Cross-validation metrics
```{r}
library(caret)

#specify the cross-validation method
# k-fold CV; k=10
ctrl <- trainControl(method = "cv", number = 10)

#fit a regression model and use k-fold CV to evaluate performance
model <- train((price) ~ bedrooms + bathrooms + sqft_living + floors + 
    condition + yr_built + I(log(sqft_lot)), data = seattle.df, method = "lm", trControl = ctrl)

#view summary of k-fold CV               
print(model)
```

Cross-validated RMSE = 144985, R2 = 0.4888, MAE = 110719.5. 

```{r}
# Plot of Actual vs. Predicted House Prices
plot(lm.pred, test.df$price, xlab="Predicted price", ylab="Actual price", main = "Actual vs. Predicted House Prices")
abline(lm(test.df$price~lm.pred,
          data=seattle.df),col="red",lwd=2)
```
### Conclusion
Our linear regression model of price resulted in a cross-validated RMSE = 144985, R2 = 0.4888, and MAE = 110719.5. RMSE = 144985 is the standard deviation of the residuals. 48%, or a bit less than half, of the variability in price is explained by the linear relationship with the predictors. MAE = 110719.5 tells us that our predictions will be off by about 110,000 dollars on average.