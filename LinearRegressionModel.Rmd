---
title: "House Price Prediction: Linear Regression and the Lasso"
output:
  word_document: default
  html_document: default
date: "2023-04-27"
---

### Introduction
In this file, we will use a linear regression model and the lasso to predict housing prices in Seattle, Washington in 2014. The data we will use is from Kaggle, uploaded by SHREE. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues. Additionally, we will model the data using random forest and cubic spline. The full dataset contains 4600 houses throughout the state of Washington with 18 variables, 1573 of the houses belonging to Seattle. The response variable is price. 

### Summary of the data set
```{r}
data <- read.csv('~/Desktop/math_448/Project/House_Price_Prediction/data.csv',
                 stringsAsFactors = TRUE)
str(data)
summary(data)
```

### Data cleaning
We immediately remove the observations with price = 0. Waterfront needs to be converted to a factor variable. The variables sqft_above and sqft_basement will be dropped, since their sum is the variable sqft_living. Also, we do not know the meaning of the view variable, so we choose to exclude it from the model. Next, we drop the following variables: date, street, statezip, and country. Including zip codes as factors would increase the number of variables greatly, so we decide not to consider them in the model. Once the data is , cleaned, we will take a subset of only the Seattle entries.
```{r}
data2 <- subset(data, price > 0)
data2$waterfront <- as.factor(data2$waterfront)
data2 <- subset(data2, select = -c(sqft_above,sqft_basement, view, 
                                   date, street, statezip, country)) 
summary(data2)
```

Now we clean the response variable: price.

```{r}
data2 <- subset(data2, price != 7800) #Low outlier found
boxplot(data2$price) #with outliers
#identify and remove outliers. R does this using the 1.5*IQR criterion
outliers <- boxplot(data2$price, plot=FALSE)$out
data2 <- data2[-which(data2$price %in% outliers),]
```
```{r}
hist(data2$price)
plot(density(log(data2$price)))
```

We notice that price has a right skew, so we will check the fit for log(price) along with price.

### Exploratory Data Analysis
```{r}
# Create Seattle data frame
seattle.df = subset(data2, city == "Seattle")
seattle.df = subset(seattle.df, select = -c(city))
dim(seattle.df)
summary(seattle.df)
```

There is only one waterfront = 1 entry, so we will remove the waterfront variable in the model. 

```{r}
# Drop waterfront
seattle.df = subset(seattle.df, select = -c(waterfront))
```

Now each predictor is numerical. The Seattle data frame consists of 1461 houses with 9 variables. 

```{r}
# Price
boxplot(seattle.df$price)
summary(seattle.df$price)
hist(seattle.df$price)
plot(density(log(seattle.df$price)))
```
From the summary: the minimum house price in Seattle is 90,000, the median is 475,000, the mean is 509351, and the maximum is 1,145,000. 

```{r}
# Relationships between response and predictors
par(mfrow = c(2, 2))
plot(price ~ bedrooms, data = seattle.df, main="Price vs Bedrooms", lwd=2)
  abline(lm(price~bedrooms, data=seattle.df),col="red",lwd=2)
plot(price ~ bathrooms, data = seattle.df, main="Price vs Bathrooms", lwd=2)
  abline(lm(price~bathrooms, data=seattle.df),col="red",lwd=2)
plot(price ~ sqft_living, data = seattle.df, main="Price vs Sqft_living", lwd=2)
  abline(lm(price~sqft_living, data=seattle.df),col="red",lwd=2)
plot(price ~ sqft_lot, data = seattle.df, main="Price vs Sqft_lot", lwd=2)
  abline(lm(price~sqft_lot, data=seattle.df),col="red",lwd=2)
plot(price ~ floors, data = seattle.df, main="Price vs Floors", lwd=2)
  abline(lm(price~floors, data=seattle.df),col="red",lwd=2)
plot(price ~ condition, data = seattle.df, main="Price vs Condition", lwd=2)
  abline(lm(price~condition, data=seattle.df),col="red",lwd=2)
plot(price ~ yr_built, data = seattle.df, main="Price vs Year built", lwd=2)
  abline(lm(price~yr_built, data=seattle.df),col="red",lwd=2)
plot(price ~ yr_renovated, data = seattle.df, main="Price vs Year renovated", lwd=2)
  abline(lm(price~yr_renovated, data=seattle.df),col="red",lwd=2)

```

There appears to be no linear relationship between price and sqft_lot, price and yr_built, and price and yr_renovated. Price is positively correlated with all other predictors. 

### Creating the model

#### Splitting data into training and test sets
```{r}
library(ggplot2)
set.seed(1)

# Training and test sets
train = sample(1:nrow(seattle.df), nrow(seattle.df)*0.8) # 80/20 split
test = (-train)
price.test = seattle.df$price[test]

# Dataframes
train.df = data.frame(seattle.df[train,])
test.df = data.frame(seattle.df[test,])
```


#### Full fit of the model
```{r}
seattle_full.lm = lm((price) ~ ., data=seattle.df, subset=train)
summary(seattle_full.lm)

# Test MSE 
lm_full.pred = predict(seattle_full.lm, test.df, type=c("response"))
err.lm_full = mean((lm_full.pred-test.df$price)^2)
err.lm_full

# Log-transformed price
seattle_full.lmlog = lm(log(price) ~ ., data=seattle.df, subset=train)
summary(seattle_full.lmlog)
```

The price model performs better than the log(price) model (R2 = 0.48 vs. R2 = 0.45). After fitting the linear regression model, we find that yr_renovated is not significant.

#### Model 2: Selection based on p-value, and best subset selection
```{r}
seattle.lm2 = update(seattle_full.lm, ~ . -yr_renovated) 
summary(seattle.lm2)

#Test MSE 
lm.pred = predict(seattle.lm2, test.df, type=c("response"))
err.lm = mean((lm.pred-test.df$price)^2)
err.lm
```

#### Model Adequacy Checking
Assumptions for linear regression: 1) little to no multicollinearity, 2) linear relationship between predictors and response, 3) normally-distributed variables, and 4) variances are equal.

```{r}
# Checking multicollinearity
pairs(seattle.df)
cor(seattle.df[,c("price","bedrooms","bathrooms", "sqft_living", "sqft_lot","floors", "condition", "yr_built", "yr_renovated")])
```

Since every coefficient is less than 0.75, we will assume there is little multicollinearity and leave the variables as they are. 

```{r}
```

Now recall from the analysis of the individual scatterplots of price against the predictors that sqft_living and yr_built have no linear relationship with price. We apply a log-transformation to these predictors.

```{r}
# Need linear relationship between response and predictors 
# Check if log-transform yields linear relationship
plot(price ~ I(log(sqft_living)), data = seattle.df, main="Price vs Sqft_living", lwd=2)
  abline(lm(price~I(log(sqft_living)), data=seattle.df),col="red",lwd=2)
plot(price ~ I(log(yr_built)), data = seattle.df, main="Price vs Year built", lwd=2)
  abline(lm(price~I(log(yr_built)), data=seattle.df),col="red",lwd=2)
```

Price and log(sqft_living) are positively correlated, so we will include log(sqft_living) instead of sqft_living.

Now we check the residuals.

```{r}
# Checking residuals
plot(seattle.lm2)
```
There is nothing unusual with the residuals. We can assume the variables are normally distributed and variances are equal.

Now we conduct a likelihood ratio test to compare nested models.

```{r}
# Likelihood ratio test
anova(seattle.lm2, seattle_full.lm, test="LRT")
```

We fail to reject H0, so it is safe to assume the simpler model and the full model are the same.

Now we select a model using backward subset selection.

#### Backward subset selection
```{r}
# Backward subset selection
library(MASS)
stepAIC(seattle_full.lm) # stepwise backward selection using AIC
```

Backward subset selection removes one predictor: yr_renovated. Thus the number of predictors is the same as in model 2. Now we will log-transform the necessary predictor, and use this as our linear regression model for prediction.

#### Model 3: Transform predictors
```{r}
#Transform sqft_living
seattle.lm3 = update(seattle_full.lm, ~ . -yr_renovated -sqft_living + I(log(sqft_living)))
summary(seattle.lm3)

# Test MSE 
lm.pred = predict(seattle.lm3, test.df, type=c("response"))
err.lm = mean((lm.pred-test.df$price)^2)
err.lm
```

### Final linear regression model
```{r}
seattle.lm3 = update(seattle_full.lm, ~ . -yr_renovated -sqft_living + I(log(sqft_living)))
summary(seattle.lm3)

# Test MSE 
lm.pred = predict(seattle.lm3, test.df, type=c("response"))
err.lm = mean((lm.pred-test.df$price)^2)
err.lm

plot(test.df$price, lm.pred)
abline(lm(lm.pred~test.df$price,
          data=seattle.df),col="red",lwd=2)
```
### Conclusion
Our linear regression model of price resulted in an R^2 of 0.4906, meaning that almost half of the variability in price is explained by a linear relationship with the predictors. Next, we will use the lasso, which will hopefully improve on the linear regression model. 
