---
title: "House Price Prediction Model 2: The Lasso"
output:
  word_document: default
  html_document: default
date: "2023-05-18"
---

### Summary of the data set
```{r}
seattle.df <- read.csv('~/Desktop/House_Price_Prediction/Seattle.csv',
                 stringsAsFactors = TRUE)
summary(seattle.df)
```

### Splitting data into training and test sets
```{r}
library(ggplot2)
set.seed(1)

# Training and test sets
train = sample(1:nrow(seattle.df), nrow(seattle.df)*0.8) # 80/20 split
test = (-train)
price.test = seattle.df$price[test]

# Dataframes
train.df = data.frame(seattle.df[train,])
test.df = data.frame(seattle.df[test,])
```

#### 4. The Lasso
```{r}
library(glmnet)
x = model.matrix(price ~ . -sqft_living + I((sqft_living)), data = seattle.df)[,-1]
y = seattle.df$price
```

We use cross-validation to find the best tuning parameter lambda, then perform the lasso. Coefficients that are shrunk to zero will be effectively removed from the final model.

```{r}
#################
# The Lasso  ####
#################
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)#coefficient plot
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)#CV error plot
bestlam=cv.out$lambda.min #cv.out$lambda.1se
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-price.test)^2) #compare to OLS error
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:9,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

The best lambda found with 10-fold cross-validation is 248. After performing the lasso, we find that none of the coefficients of the predictors were shrunk to zero. yr_renovated and sqft_lot are relatively close to 0 compared to the others, with coefficients of 3.6 and -5.2 respectively. 

```{r}
# RMSE
rootErr = sqrt(mean((lasso.pred-price.test)^2))
rootErr
```

The root mean squared error is 146391.20, which is slightly lower than the error from the multiple linear regression model (149852.57). 

```{r}
# R-squared
tss <- sum((price.test - mean(price.test))^2)
rss <- sum((lasso.pred - price.test)^2)

rsq <- 1 - rss/tss
rsq
```

The lasso model R2 = 0.51, so a little more than half of the variation in price is explained by the linear relationship with the predictors. This is a slight improvement from the multiple linear regression model R2 = 0.49.
