---
title: "new_model"
author: "kaiyu"
date: "2023-04-14"
output: html_document
---
### Introduction
In this file, we will use a random forest model to predict housing prices in Seattle, Washington in 2014. The data we will use is from Kaggle, uploaded by SHREE. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues. Additionally, we will model the data using random forest and cubic spline. The full dataset contains 4600 houses throughout the state of Washington with 18 variables, 1573 of the houses belonging to Seattle. The response variable is price. 
###importing a dataset
```{r}
#importing dataset
data <- read.csv('~/Desktop/House_Price_Prediction/Seattle_with_renovated.csv')
summary(data)
str(data)
```

###splitting into training and test dataset
```{r}
library(rpart)
library(randomForest)
set.seed(123)  # for reproducibility
train_index <- sample(1:nrow(data), 0.8 * nrow(data))
train <- data[train_index, ]
test <- data[-train_index, ]
```


###convert yr_built to age of house, price to log price and set hyperparameters for random forest
```{r}
library(randomForest)
# Create new features in both train and test sets
train$age_of_house <- max(train$yr_built) - train$yr_built
train$renovated <- as.integer(train$yr_renovated > 0)  # binary indicator of renovation
train$yr_built <- NULL
train$yr_renovated <- NULL

test$age_of_house <- max(test$yr_built) - test$yr_built
test$renovated <- as.integer(test$yr_renovated > 0)  # binary indicator of renovation
test$yr_built <- NULL
test$yr_renovated <- NULL

# Log-transform the target variable
train$log_price <- log(train$price)
test$log_price <- log(test$price)
head(train$log_price)
head(train$price)
train <- subset(train, select = -price)
test <- subset(test, select = -price)
# Set hyperparameters
mtry <- sqrt(ncol(train) - 1) # mtry is square root of the number of features
mtry
ntree <- 500 
ntree
```


###train random forest
```{r}
# Train the Random Forest model
tree <- randomForest(log_price ~ ., data=train, mtry=mtry, ntree=ntree, importance=TRUE)
print(tree)

tree_predictions <- predict(tree, newdata=test)

tree_mse <- mean((test$log_price - tree_predictions)^2)
print(paste("Random Forest MSE: ", tree_mse))
```

###find the best mse by trying numbers of mtry and ntree. 
```{r}
# Hyperparameter tuning
mtry_values <- c(sqrt(ncol(train) - 1), sqrt(ncol(train) - 1) - 1, sqrt(ncol(train) - 1) + 1)
ntree_values <- c(500, 1000, 1500)
best_mse <- Inf
best_mtry <- NA
best_ntree <- NA

for (mtry in mtry_values) {
  for (ntree in ntree_values) {
    set.seed(123)
    model <- randomForest(log_price ~ ., data=train, mtry=mtry, ntree=ntree, importance=TRUE)
    predictions <- predict(model, newdata=test)
    mse <- mean((test$log_price - predictions)^2)
    if (mse < best_mse) {
      best_mse <- mse
      best_mtry <- mtry
      best_ntree <- ntree
    }
  }
}

print(paste("Best MSE: ", best_mse))
print(paste("Best mtry: ", best_mtry))
print(paste("Best ntree: ", best_ntree))
```


###use the best model from above.
```{r}
set.seed(123)
final_model <- randomForest(log_price ~ ., data=train, mtry=best_mtry, ntree=best_ntree, importance=TRUE)
final_predictions <- predict(final_model, newdata=test)
final_mse <- mean((test$log_price - final_predictions)^2)
final_rmse <- sqrt(final_mse)
print(paste("Final RMSE: ", final_rmse))
```


###cross validation

```{r}
# Number of folds
k <- 5

# Create k equally size folds
folds <- cut(seq(1, nrow(train)), breaks=k, labels=FALSE)

mse_values <- vector("numeric", k)
for(i in 1:k){
  test_indices <- which(folds==i, arr.ind=TRUE)
  test_data <- train[test_indices, ]
  train_data <- train[-test_indices, ]
  set.seed(123)
  model <- randomForest(log_price ~ ., data=train_data, mtry=best_mtry, ntree=best_ntree, importance=TRUE)
  
  # Make predictions
  predictions <- predict(model, newdata=test_data)
  
  # Compute MSE
  mse_values[i] <- mean((test$log_price - predictions)^2)
  
}

average_mse <- mean(mse_values)
average_rmse <- sqrt(average_mse)

print(paste("Average MSE from 5-fold CV: ", average_mse))
print(paste("Average RMSE from 5-fold CV: ", average_rmse))

```

considering price range is relatively big, this model can be said pretty robust. 

###plotting the best model 
```{r}
# Final model predictions
final_model <- randomForest(log_price ~ ., data=train, mtry=best_mtry, ntree=best_ntree, importance=TRUE)
final_predictions <- predict(final_model, newdata=test)
summary(final_model)
plot(test$log_price, final_predictions, main="True vs Predicted Values", xlab="True Values", ylab="Predicted Values")
abline(0, 1, col="red")
residuals <- test$log_price - final_predictions
plot(final_predictions, residuals, main="Residuals vs Fitted Values", xlab="Fitted Values", ylab="Residuals")
abline(h=0, col="red") 
```
```{r}
importance(final_model)
varImpPlot(final_model)
```


